---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.4
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
from CitizenUAV.models import InatSequentialClassifier
from CitizenUAV.data import InatDataModule
from CitizenUAV.processes import optimize_image_resnet
from torch import nn
from torchvision.transforms.functional import to_pil_image
from matplotlib import pyplot as plt
from CitizenUAV.io_utils import get_pid_from_path
import torch
from torch import optim
from tqdm import tqdm
import numpy as np
import pandas as pd
import os
```

```{python}
result_dir = '/home/ndettmer/experiments/ma/classification/maize_ternary/predictions/style_transfer/'
dm = InatDataModule('/home/ndettmer/ssd_data/inat_subsets/maize_ternary_manual_preselect_old', normalize=False, batch_size=1, min_distance=5, return_path=True)
norm = dm.get_normalize_module()
model_version = 'version_4'
cnn = InatSequentialClassifier.load_from_checkpoint(f'/home/ndettmer/experiments/ma/classification/maize_ternary_manual_preselect/lightning_logs/{model_version}/checkpoints/epoch=49-step=43200.ckpt')
cnn.eval()

pred_df = pd.read_csv(f'/home/ndettmer/experiments/ma/classification/maize_ternary_manual_preselect_old/predictions/inat/{model_version}/23-05-18_15-21_predictions.csv')
```

```{python}
def get_safe_pred_sample(label):
    subset = pred_df[(pred_df.target_text == label) & (pred_df.prediction_text == label)]
    max_prob = subset[f"{label}_prob"].max()
    subset = subset[subset[f"{label}_prob"] == max_prob]
    if len(subset) > 0:
        return subset.sample()
    print(f"No sample found for label {label}")
    return None
```

```{python}
maize_sample_pid = get_safe_pred_sample('zea mays').pid.values[0]
maize_sample, _, _ = dm.ds.dataset.get_item_by_pid(maize_sample_pid)

weed_sample_pid = get_safe_pred_sample('weed').pid.values[0]
weed_sample, _, _ = dm.ds.dataset.get_item_by_pid(weed_sample_pid)

soil_sample_pid = get_safe_pred_sample('soil').pid.values[0]
soil_sample, _, _ = dm.ds.dataset.get_item_by_pid(soil_sample_pid)
```

```{python}
maize_sample_pid, weed_sample_pid, soil_sample_pid
```

```{python}
plt.figure(figsize=(15, 15))
plt.subplot(3, 2, 1)
plt.imshow(to_pil_image(maize_sample))
plt.subplot(3, 2, 2)
plt.imshow(to_pil_image(norm(maize_sample)))
plt.subplot(3, 2, 3)
plt.imshow(to_pil_image(weed_sample))
plt.subplot(3, 2, 4)
plt.imshow(to_pil_image(norm(weed_sample)))
plt.subplot(3, 2, 5)
plt.imshow(to_pil_image(soil_sample))
plt.subplot(3, 2, 6)
plt.imshow(to_pil_image(norm(soil_sample)))
plt.show()
```

![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*tH9evuOFqk8F41FG.png)


I want to visualize the outputs of Stages 2, 3, 4 and 5.
- Stage 2: `model.feature_extractor[4]`
- Stage 3: `model.feature_extractor[5]`
- Stage 4: `model.feature_extractor[6]`
- Stage 5: `model.feature_extractor[7]`

<!-- #region jp-MarkdownHeadingCollapsed=true tags=[] jp-MarkdownHeadingCollapsed=true jp-MarkdownHeadingCollapsed=true tags=[] jp-MarkdownHeadingCollapsed=true -->
## Content and Style
<!-- #endregion -->

Stage 3 Seems to be better for content representation! Better segmentation!


### Maize

```{python}
output = optimize_image_resnet(cnn, [4, 5], dm.get_normalize_module(), maize_sample, 'ContentLoss')
content_repr = to_pil_image(output.squeeze(0))

plt.figure(figsize=(10, 10))
plt.subplot(1, 2, 1)
plt.imshow(content_repr)
plt.subplot(1, 2, 2)
plt.imshow(to_pil_image(maize_sample))
plt.savefig(f'{result_dir}/{maize_sample_pid}_{model_version}_content_repr.png')
```

```{python}
output = optimize_image_resnet(cnn, [4, 5, 6, 7], dm.get_normalize_module(), maize_sample, 'StyleLoss', num_steps=500)
style_repr = to_pil_image(output.squeeze(0))
plt.figure()
plt.imshow(style_repr)
plt.savefig(f'{result_dir}/{maize_sample_pid}_{model_version}_style_repr.png')
```

### Weed

```{python}
output = optimize_image_resnet(cnn, [4, 5], dm.get_normalize_module(), weed_sample, 'ContentLoss')
content_repr = to_pil_image(output.squeeze(0))

plt.figure(figsize=(10, 10))
plt.subplot(1, 2, 1)
plt.imshow(content_repr)
plt.subplot(1, 2, 2)
plt.imshow(to_pil_image(weed_sample))
plt.savefig(f'{result_dir}/{weed_sample_pid}_{model_version}_content_repr.png')
```

```{python}
output = optimize_image_resnet(cnn, [4, 5, 6, 7], dm.get_normalize_module(), weed_sample, 'StyleLoss', num_steps=500)
style_repr = to_pil_image(output.squeeze(0))
plt.figure()
plt.imshow(style_repr)
plt.savefig(f'{result_dir}/{weed_sample_pid}_{model_version}_style_repr.png')
```

### Soil

```{python}
output = optimize_image_resnet(cnn, [4, 5], dm.get_normalize_module(), soil_sample, 'ContentLoss')
content_repr = to_pil_image(output.squeeze(0))

plt.figure(figsize=(10, 10))
plt.subplot(1, 2, 1)
plt.imshow(content_repr)
plt.subplot(1, 2, 2)
plt.imshow(to_pil_image(soil_sample))
plt.savefig(f'{result_dir}/{soil_sample_pid}_{model_version}_content_repr.png')
```

```{python}
output = optimize_image_resnet(cnn, [4, 5, 6, 7], dm.get_normalize_module(), soil_sample, 'StyleLoss', num_steps=500)
style_repr = to_pil_image(output.squeeze(0))
plt.figure()
plt.imshow(style_repr)
plt.savefig(f'{result_dir}/{soil_sample_pid}_{model_version}_style_repr.png')
```

<!-- #region jp-MarkdownHeadingCollapsed=true tags=[] jp-MarkdownHeadingCollapsed=true -->
## Feature Maps
<!-- #endregion -->

```{python}
stage_2 = nn.Sequential(*list(cnn.feature_extractor[:5]))
```

```{python}
stage_2.eval()
with torch.no_grad():
    fms = stage_2(maize_sample.unsqueeze(0))
```

```{python}
fms = fms.squeeze(0)
n_maps = fms.shape[0]
ax_len = int(np.sqrt(n_maps))

fig, axs = plt.subplots(ax_len, ax_len, figsize=(20, 20))
for i in range(n_maps):
    fm = fms[i]
    axs[i % ax_len, i // ax_len].imshow(to_pil_image(fm))
    axs[i % ax_len, i // ax_len].set_title(i)
    
plt.show()
```

```{python}
max_activation = torch.argmax(torch.sum(fms, dim=(1,2)))
```

```{python}
max_activation
```

```{python}
stage_3 = nn.Sequential(*list(cnn.feature_extractor[:6]))
```

```{python}
stage_3.eval()
with torch.no_grad():
    fms = stage_3(maize_sample.unsqueeze(0))
```

```{python}
fms = fms.squeeze(0)
n_maps = fms.shape[0]
ax_len = int(np.ceil(np.sqrt(n_maps)))

fig, axs = plt.subplots(ax_len, ax_len, figsize=(20, 20))
for i in range(n_maps):
    fm = fms[i]
    try:
        axs[i % ax_len, i // ax_len].imshow(to_pil_image(fm))
        axs[i % ax_len, i // ax_len].set_title(i)
    except IndexError:
        print(i)
        raise
    
plt.show()
```

```{python}
max_activation = torch.argmax(torch.sum(fms, dim=(1,2)))
```

```{python}
max_activation
```

NOTE: Let's try neuron 39 of stage 2 for now. 


## Captum

```{python}
from captum.attr import IntegratedGradients
from captum.attr import visualization as viz
from matplotlib.colors import LinearSegmentedColormap
```

<!-- #region tags=[] -->
### Integrated Gradient Playground
<!-- #endregion -->

```{python}
model = cnn
_ = model.eval()
```

```{python}
transformed_img = maize_sample.unsqueeze(0)
input = norm(transformed_img)
output = model(input)
```

```{python}
prediction_score, pred_label_idx = torch.topk(output, 1)
predicted_label = dm.ds.classes[pred_label_idx.item()]
```

```{python}
integrated_gradients = IntegratedGradients(model)
```

```{python}
attributions_ig = integrated_gradients.attribute(input, target=pred_label_idx, n_steps=200)
```

```{python}
default_cmap = LinearSegmentedColormap.from_list('custom blue', 
                                                 [(0, '#ffffff'),
                                                  (0.25, '#000000'),
                                                  (1, '#000000')], N=256)

_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),
                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),
                                      ["original_image", "masked_image"],
                                      ["all", "positive"],
                                      cmap=default_cmap,
                                      show_colorbar=True,
                                      outlier_perc=2,
                                     )
```

### Occlusion

```{python}
from captum.attr import Occlusion
```

```{python}
occlusion = Occlusion(model)
attributions_occ = occlusion.attribute(
    input,
    strides=(3, 4, 4),
    target=pred_label_idx,
    sliding_window_shapes=(3, 10, 10),
    baselines=0,
    show_progress=True
)
```

```{python}
_ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),
                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),
                                      ["original_image", "heat_map"],
                                      ["all", "positive"],
                                      show_colorbar=True,
                                      outlier_perc=2,
                                     )
```

```{python}
pred_label_idx
```

### GradCAM

```{python}
n_samples = 10
pred_df['prob_std'] = pred_df[['soil_prob', 'weed_prob', 'zea mays_prob']].std(1)
unconfident_samples = pred_df.loc[pred_df.prob_std.nsmallest(n_samples).index]
```

```{python}
unconfident_samples
```

```{python}
us = unconfident_samples.sample()
us_image, *_ = dm.ds.dataset.get_item_by_pid(us.pid.values[0])
```

```{python}
transformed_img = us_image.unsqueeze(0)
input = norm(transformed_img)
output = model(input)
```

```{python}
prediction_score, pred_label_idx = torch.topk(output, 1)
predicted_label = dm.ds.classes[pred_label_idx.item()]
```

```{python}
target_layer = [l for l in list(model.feature_extractor[-1][-1].children()) if isinstance(l, nn.Conv2d)][-1]
```

```{python}
from captum.attr import GuidedGradCam
g_grad_cam = GuidedGradCam(model, target_layer)
```

```{python}
attributions_cams = [g_grad_cam.attribute(input.requires_grad_(), t) for t in range(len(dm.ds.classes))]
```

```{python}
for attributions_cam in attributions_cams: 
    _ = viz.visualize_image_attr_multiple(np.transpose(attributions_cam.squeeze().cpu().detach().numpy(), (1,2,0)),
                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),
                                      ["original_image", "masked_image"],
                                      ["all", "positive"],
                                      show_colorbar=True,
                                      outlier_perc=2,
                                     )
```

```{python}
pred_scores = model(input)
```

```{python}
pred_scores.squeeze()[0].item()
```

```{python}
fig = viz.visualize_image_attr_multiple(np.transpose(attributions_cams[0].squeeze().cpu().detach().numpy(), (1,2,0)),
                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),
                                      ["original_image", "masked_image"],
                                      ["all", "positive"],
                                      show_colorbar=True,
                                      outlier_perc=2,
                                      use_pyplot=False
                                     )[0]

```

```{python}
fig.set_title("Hey")
```

```{python}

```
