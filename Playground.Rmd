---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
import threading
import torch
import logging
import time
from torchvision.models import resnet50
```

```{python}
cuda = torch.device('cuda')
cpu = torch.device('cpu')
```

```{python}
def print_mem_usage():
    print(f"Currently reserved GPU memory: {torch.cuda.memory_reserved() / 1000000} MB")
    print(f"Currently allocated GPU memory: {torch.cuda.memory_allocated() / 1000000} MB")
```

```{python}
def load_tensor():
    model = resnet50()
    model = model.cuda()
    print_mem_usage()
    time.sleep(2)
    x = torch.randn((1, 3, 256, 256))
    x = x.cuda()
    print_mem_usage()
    time.sleep(2)
    
    y = model(x)
    print_mem_usage()
    time.sleep(2)
    
    del model
    del x
    del y
    
    print_mem_usage()
    time.sleep(2)
    torch.cuda.empty_cache()
    print_mem_usage()
```

```{python}
t1 = threading.Thread(target=load_tensor)
t2 = threading.Thread(target=load_tensor)

t1.start()
#t2.start()
```

```{python}
torch.cuda.empty_cache()
```

```{python}
print(f"Currently used GPU memory: {torch.cuda.memory_allocated() / 1000000} MB")
print(f"Currently reserved GPU memory: {torch.cuda.memory_reserved() / 1000000} MB")
print(f"Currently allocated GPU memory: {torch.cuda.memory_allocated() / 1000000} MB")
```

```{python}
model = resnet50()
model = model.cuda()
```

```{python}
print_mem_usage()
model = resnet50()
model.cuda()
print_mem_usage()
del model
torch.cuda.empty_cache()
x = torch.randn((8, 3, 256, 256)).float().cuda()
print_mem_usage()
```

```{python}
del x
torch.cuda.empty_cache()
```

```{python}
8000 / 20
```

```{python}
import pandas as pd
```

```{python}
metadata = pd.read_csv('/home/ndettmer/data/inat/metadata.csv')
metadata.photo_id = metadata.photo_id.astype(str)
metadata.to_csv('/home/ndettmer/data/inat/metadata.csv', index=False)
```

```{python}
len(metadata[~metadata.distance.isna()]) / len(metadata)
```

```{python}
binary_cls_metadata = metadata.copy()
```

```{python}
binary_cls_metadata.columns
```

```{python}
binary_cls_metadata[binary_cls_metadata.species == 'reynoutria japonica'].sample().label
```

```{python}
binary_cls_metadata[binary_cls_metadata.species != 'reynoutria japonica'].species = 'not_rjaponica'
```

```{python}
binary_cls_metadata.species.unique()
```

```{python}
binary_cls_metadata.species = binary_cls_metadata.species.astype('category')
binary_cls_metadata.label = binary_cls_metadata.species.cat.codes
```

```{python}
binary_cls_metadata.label.unique()
```

```{python}
binary_cls_metadata[binary_cls_metadata.label == 1].species.unique()
```

```{python}
binary_cls_metadata.to_csv('/home/ndettmer/data/inat_subsets//metadata.csv')
```

```{python}
test_df = binary_cls_metadata.copy()
test_df.photo_id = binary_cls_metadata.photo_id.astype(str)
```

```{python}
test_df.set_index('photo_id', inplace=True)
```

```{python}
test_df.sample()
```

```{python}
test_df.loc[str(97473710)].copy()
```

```{python}
dist_df = pd.read_csv('/home/ndettmer/data/distance_regression/mixed/distances.csv')
```

```{python}
cs1_metadata = pd.read_csv('/home/ndettmer/data/inat_subsets/case_study_1/metadata.csv')
```

```{python}
cs1_metadata.columns
```

```{python}
cs1_metadata.drop(columns=['Unnamed: 0'], inplace=True)
```

```{python}
cs1_metadata.to_csv('/home/ndettmer/data/inat_subsets/case_study_1/metadata.csv', index=False)
```

```{python}
from CitizenUAV.processes import check_image_files

check_image_files('/home/ndettmer/data/inat_subsets/case_study_1')
```

TODO: 
- [ ] check images that are not okay (~ 25,999 should be not okay, because i removed reynoutria japonica from not_rj)
- [ ] remove image files that are not okay if the numbers are correct, otherwise rebuild case study 1 metadata

```{python}
len(cs1_metadata[cs1_metadata.image_okay == 'False'])
```

```{python}
cs1_metadata.species.unique()
```

## Revert old augmentation processes

```{python}
import os
import pandas as pd
from tqdm import tqdm
```

### Distance dataset

```{python}
data_dir = '/home/ndettmer/data/distance_regression/'
distances = pd.read_csv(os.path.join(data_dir, 'mixed/distances.csv'))
```

```{python}
distances.columns
```

```{python}
def detect_augmented(x):
    return 'augmented' in x.Image

tqdm.pandas()
augmented = distances.progress_apply(detect_augmented, axis=1)
```

```{python}
sum(augmented)
```

```{python}
exists = distances[augmented].apply(lambda x: os.path.exists(x.path), axis=1)
sum(exists)
```

```{python}
for idx, row in distances[augmented].iterrows():
    os.remove(row.path)
```

```{python}
distances.exists = distances.apply(lambda x: os.path.exists(x.path), axis=1)
```

```{python}
distances = distances[distances.exists]
```

```{python}
distances.to_csv(os.path.join(data_dir, 'mixed/distances.csv'), index=False)
```

```{python}
from torchvision.datasets import ImageFolder
```

```{python}
ds = ImageFolder(data_dir)
for i in range(len(ds)):
    path, _ = ds.samples[i]
    if 'augmented' in path:
        df_idx = distances[distances.Image == os.path.basename(path)].index
        distances.drop(index=df_idx, inplace=True)
        #os.remove(path)
```

```{python}
len(distances)
```

### iNat dataset

```{python}
data_dir = '/home/ndettmer/data/inat/'
metadata = pd.read_csv(os.path.join(data_dir, 'metadata.csv'))
```

```{python}
metadata.columns
```

```{python}
def detect_augmented(x):
    return str(x.photo_id).count('_') > 1

tqdm.pandas()
augmented = metadata.progress_apply(detect_augmented, axis=1)
```

```{python}
sum(augmented)
```

## Combine metadata

```{python}
cs1_dir = '/home/ndettmer/data/inat_subsets/case_study_1'
gim_dir = '/home/ndettmer/data/gim_images'

metadata = pd.read_csv(os.path.join(cs1_dir, 'metadata.csv'))
gim_metadata = pd.read_csv(os.path.join(gim_dir, 'metadata.csv'))
```

```{python}
gim_metadata.species = 'river'
```

```{python}
gim_metadata.species.unique()
```

```{python}
new_metadata = pd.concat([metadata, gim_metadata])
```

```{python}
metadata.reset_index(inplace=True)
```

```{python}
metadata.photo_id = metadata.photo_id.astype(str)
```

```{python}
new_metadata.species = new_metadata.species.astype('category')
new_metadata.label = new_metadata.species.cat.codes
```

```{python}
new_metadata.drop(columns=['Unnamed: 0'], inplace=True)
```

```{python}
new_metadata.columns
```

```{python}
new_metadata.label.unique()
```

```{python}
new_metadata.to_csv(os.path.join(cs1_dir, 'metadata.csv'))
```

```{python}
import numpy as np
from collections import Counter

idx = np.array(range(100))
targets = np.random.choice([0, 1, 2], size=100, replace=True)

n_samples = dict(Counter(targets))

min_n = min(n_samples.values())
```

```{python}
new_idx = []
```

```{python}
np.unique(targets)
```

```{python}
#for t in n_samples.keys():
#    new_idx[t] = np.random.choice(np.argwhere(targets == t).flatten(), size=min_n, replace=False)
    
new_idx = [np.random.choice(np.argwhere(targets == t).flatten(), size=min_n, replace=False) for t in n_samples.keys()]
```

```{python}
len(np.concatenate(new_idx)) == 3 * 27
```

```{python}
n_samples
```

```{python}
min_n
```

```{python}
total_set = set(range(100))
idx = set(np.random.choice(range(100), size=50, replace=False))
```

```{python}
not_idx = total_set ^ idx
```

```{python}
from CitizenUAV.utils import read_inat_metadata
df = read_inat_metadata('/home/ndettmer/data/inat_subsets/case_study_1/')
```

```{python}
'image_okay' in df
```

```{python}
from time import time
import multiprocessing as mp
from CitizenUAV.data import InatDataModule
from torch.utils.data import DataLoader
from tqdm import tqdm

dm = InatDataModule('/home/ndettmer/data/inat_subsets/case_study_1', batch_size=10, min_distance=5, balance=True)
dm.setup()

for num_workers in [2, 4, 8]:  
    train_loader = DataLoader(dm.train_ds, num_workers=num_workers, batch_size=dm.batch_size, pin_memory=True)
    start = time()
    for epoch in range(2):
        pbar = tqdm(train_loader)
        pbar.set_description(f"Epoch {epoch}")
        for i, data in pbar:
            pass
    end = time()
    print("Finish with:{} second, num_workers={}".format(end - start, num_workers))
```

```{python}
mp.cpu_count()
```

# RasterIO

```{python}
import rasterio as rio
import rasterio.mask
import fiona
import os
import sys
from PIL import Image
from matplotlib import pyplot as plt
from matplotlib import patches as patches
import matplotlib.patheffects as PathEffects
import numpy as np
from torchvision.transforms import ToTensor
from torchvision.transforms.functional import to_pil_image
import torch
from CitizenUAV.data import GTiffDataset
from tqdm import tqdm
```

```{python}
data_dir = '/home/ndettmer/ssd_data/UOS_Data/'
filename = os.path.join(data_dir, '220518_maize_rgb.tif')
shape_dir = os.path.join(data_dir, '1805_maize')
maize_shape_path = os.path.join(shape_dir, '1805_maize_maize.shp')

gt_ds = GTiffDataset(filename, shape_dir, stride=32)
```

```{python}
img = gt_ds.rds.read((1, 2, 3))
gray_img = img.mean(axis=0)
labelled_area = gt_ds.get_labelled_area()
img[:, (gt_ds.get_mask_bool() & ~labelled_area)] = gray_img[(gt_ds.get_mask_bool() & ~labelled_area)]
del gray_img
```

```{python}
n = 25
side_len = int(np.sqrt(n))
figure, axs = plt.subplots(side_len, side_len, figsize=(20, 20))
c = np.random.choice(range(gt_ds.bbs.shape[0]), size=n)
for i, idx in enumerate(c):
    bb = gt_ds.bbs[idx]
    x_min, x_max, y_min, y_max = bb
    #print(x_min, x_max, y_min, y_max)
    sample_img = to_pil_image(torch.from_numpy(img[:, x_min:x_max, y_min:y_max]))
    axs[i % side_len, i // side_len].imshow(sample_img)
    axs[i % side_len, i // side_len].set_title(f"({x_min}, {y_min}):({x_max}, {y_max})")
plt.show()
#plt.savefig(f'/home/ndettmer/Pictures/220518_maize_labelled_subset_samples.png')
```

```{python}
figure, ax = plt.subplots(figsize=(20,20))
ax.imshow(to_pil_image(torch.from_numpy(img)))
for i in c:
    bb = gt_ds.bbs[i]
    x_min, x_max, y_min, y_max = bb
    txt = ax.text(y_min - 5, x_min - 5, f"({x_min},{y_min}):({x_max},{y_max})", color='white', fontsize='6')
    txt.set_path_effects([PathEffects.withStroke(linewidth=2, foreground='black')])
    box = patches.Rectangle((y_min, x_min), 128, 128, linewidth=1, edgecolor='r', facecolor='none')
    ax.add_patch(box)
plt.show()
#plt.savefig('/home/ndettmer/Pictures/220518_maize_labelled_subset.png')
```

```{python}
with fiona.open(maize_shape_path) as maize_shapefile:
    maize_shapes = [feature["geometry"] for feature in maize_shapefile]
ds = rio.open(filename)
out_image, out_transform = rio.mask.mask(ds, maize_shapes, crop=True)
ds_transform = ds.transform
out_meta = ds.meta
bounds = ds.bounds
```

```{python}
# reference image
ref_img = ds.read(4) > 0
ref_mask, _ = rio.mask.mask(ds, maize_shapes, crop=False)
ref_mask = ref_mask[3] > 0
ref_img ^= ref_mask
```

```{python}
min_p_geo = out_transform * (0, 0)
max_p_geo = out_transform * (out_image[0].shape[1], out_image[0].shape[0])
min_p_ds = ~ds_transform * min_p_geo
max_p_ds = ~ds_transform * max_p_geo
print(min_p_ds)
print(max_p_ds)

figure, ax = plt.subplots()
ax.imshow(ref_img, cmap='gray')
box = patches.Rectangle((min_p_ds[0], min_p_ds[1]), max_p_ds[0] - min_p_ds[0], max_p_ds[1] - min_p_ds[1], linewidth=1, edgecolor='r', facecolor='none')
ax.add_patch(box)
ax.plot([min_p_ds[0], max_p_ds[0]], [min_p_ds[1], max_p_ds[1]], 'X')
plt.show()
```

```{python}
plt.imshow(out_image[3] > 0, cmap='gray')
```

```{python}
shape_mask = np.zeros((ds.height, ds.width), dtype=bool)
idxs = np.argwhere(out_image[3] > 0)
idxs = (idxs + tuple(reversed(min_p_ds))).astype(int)
shape_mask[idxs[:, 0], idxs[:, 1]] = True

plt.imshow(shape_mask, cmap='gray')
```

```{python}
plt.imshow(ref_mask, cmap='gray')
```

```{python}
(ref_mask == shape_mask).all()
```

```{python}
ref_img[idxs[0]]
```

```{python}
out_image[3].shape
```

```{python}
idxs
```

```{python}
from datetime import datetime
mask_cpy1 = shape_mask.copy()
mask_cpy2 = shape_mask.copy()

start = datetime.now()
mask_cpy1[idxs] = True
print(datetime.now() - start)

start = datetime.now()
mask_cpy2[idxs] += True
print(datetime.now() - start)
```

```{python}
x_min = 3500
x_max = x_min + 128
y_min = 6500
y_max = y_min + 128
plt.figure(figsize=(10, 10))
window = out_image[3][x_min:x_max, y_min:y_max] // 255
print(np.sum(window) / (128 ** 2))
#plt.imshow(out_image[3][x_min:x_max, y_min:y_max] // 255, cmap='gray')
plt.imshow(window, cmap='gray')
```

```{python}
mask = out_image > 0
```

```{python}
import sys
sys.getsizeof(mask)
```

```{python}
shape_dir = '/home/ndettmer/ssd_data/UOS_Data/1805_maize'
```

```{python}
for root, directory, files in os.walk(shape_dir):
    for file in files:
        if os.path.splitext(file)[1] == ".shp":
            print(os.path.splitext(file.split("_")[-1])[0])
    
```

```{python}
maize_shapes
```

```{python}
gt_ds.bbs
```

```{python}
gt_ds.bbs
```

```{python}
plt.imshow(to_pil_image(gt_ds[len(gt_ds)]))
```

```{python}
for i in tqdm(range(len(gt_ds))):
    _ = gt_ds[i]
```

```{python}
ds = rio.open(filename)
```

```{python}
ds.count
```

```{python}
ds.width
```

```{python}
ds.height
```

```{python}
type(ds)
```

```{python}
{i: dtype for i, dtype in zip(ds.indexes, ds.dtypes)}
```

```{python}
ds.bounds
```

```{python}
red = ds.read(1)
green = ds.read(2)
blue = ds.read(3)
mask = ds.read(4)
```

```{python}
x_min = ds.width // 2
y_min = ds.height // 2
x_max = x_min + 128
y_max = y_min + 128
s = np.stack([
    red[x_min:x_max, y_min:y_max],
    green[x_min:x_max, y_min:y_max],
    blue[x_min:x_max, y_min:y_max]
], axis=-1)
```

```{python}
s.shape
```

```{python}
img = Image.fromarray(s)
plt.imshow(img)
```

```{python}
ToTensor()(s).shape
```

```{python}
np.unique(mask)
```

```{python}
figure, ax = plt.subplots()
ax.imshow(Image.fromarray((mask // 255)), cmap='gray')
box = patches.Rectangle((x_min, y_min), 128, 128, linewidth=1, edgecolor='r', facecolor='none')
ax.add_patch(box)
plt.show()
```

```{python}
np.argwhere(mask[:, 0] == 255)
```

```{python}
np.argwhere(mask[0, :] == 255)
```

```{python}
np.argwhere(mask[:, -1] == 255)
```

```{python}
np.argwhere(band4[-1, :] == 255)
```

```{python}
from rasterio.windows import get_data_window, Window
```

```{python}
x_min = ds.width // 2
x_max = x_min + 128
y_min = ds.height // 2
y_max = y_min + 128
w = Window.from_slices((x_min, x_max), (y_min, y_max))
```

```{python}
w
```

```{python}
w_data = ds.read((1, 2, 3), window=w)
```

```{python}
x = torch.from_numpy(w_data)
```

```{python}
to_pil_image(x)
```

```{python}
dw = get_data_window(ds.read((1, 2, 3), masked=True))
```

```{python}
print(dw.width, ds.width)
print(dw.height, ds.height)
```

```{python}
next(ds.block_windows(1))
```

```{python}

```
