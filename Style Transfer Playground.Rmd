---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.4
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
from CitizenUAV.models import InatClassifier
from CitizenUAV.data import InatDataModule
from CitizenUAV.losses import ContentLoss
from torch import nn
from torchvision.transforms.functional import to_pil_image
from matplotlib import pyplot as plt
from CitizenUAV.io_utils import get_pid_from_path
import torch
from torch import optim
from tqdm import tqdm
```

```{python}
model_path = '/home/ndettmer/experiments/ma/classification/maize_ternary/lightning_logs/version_0/checkpoints/epoch=49-step=43200.ckpt'
cnn = InatClassifier.load_from_checkpoint(model_path)
```

```{python}
dm = InatDataModule('/home/ndettmer/data/inat_subsets/maize_ternary', normalize=True, batch_size=1, min_distance=5)
non_norm_dm = InatDataModule('/home/ndettmer/data/inat_subsets/maize_ternary', normalize=False, batch_size=1, min_distance=5)
```

```{python}
sample_pid = '175981006'
maize_sample = None
sample_idx = None
for idx, (path, t) in enumerate(dm.ds.samples):
    print(path)
    pid = get_pid_from_path(path)
    if pid == sample_pid:
        maize_sample = dm.ds[idx][0]
        sample_idx = idx
```

![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*tH9evuOFqk8F41FG.png)


I want to visualize the outputs of Stages 2, 3, 4 and 5.
- Stage 2: `model.feature_extractor[4]`
- Stage 3: `model.feature_extractor[5]`
- Stage 4: `model.feature_extractor[6]`
- Stage 5: `model.feature_extractor[7]`

```{python}
stage_2 = nn.Sequential(*list(cnn.feature_extractor.children())[:5])
stage_3 = nn.Sequential(*list(cnn.feature_extractor.children())[:6])
stage_4 = nn.Sequential(*list(cnn.feature_extractor.children())[:7])
stage_5 = nn.Sequential(*list(cnn.feature_extractor.children())[:8])
```

```{python}
# normalization is done when loading the image, so this can be ignored here

def get_content_loss(cnn_slice, content_img):
    target = cnn_slice(content_img.unsqueeze(0)).detach()
    content_loss = ContentLoss(target)
    cnn_slice.add_module('content_loss', content_loss)
    
    return cnn_slice, content_loss
```

```{python}
def get_input_optimizer(input_img):
    optimizer = optim.LBFGS([input_img])
    return optimizer
```

```{python}
def approximate_content(cnn_slice, content_img, x, num_steps=300):
    model, content_loss = get_content_loss(cnn_slice, content_img)
    x.requires_grad_(True)
    model.requires_grad_(True)
    
    optimizer = get_input_optimizer(x)
    
    
    pbar = tqdm(range(num_steps))
    pbar.set_description(f"Optimizing input image")
    for i in pbar:
        
        def closure():
            with torch.no_grad():
                x.clamp_(0, 1)

            optimizer.zero_grad()
            model(x)
            loss = content_loss.loss
            loss.backward()

            return content_loss.loss
        
        optimizer.step(closure)
        
    with torch.no_grad():
        x.clamp_(0, 1)
        
    return x
```

```{python}
x = torch.rand(maize_sample.shape).unsqueeze(0)
```

```{python}
output = approximate_content(stage_2, maize_sample, x)
```

```{python}
out_img = to_pil_image(output.squeeze(0))
plt.imshow(out_img)
```

```{python}

```
